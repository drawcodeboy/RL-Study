{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0003\n",
    "GAMMA = 0.99\n",
    "EPSILON_CLIP = 0.2\n",
    "ENTROPY_COEFF = 0.01\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64 # Replay Buffer에 있는 거 64개씩 가져오겠다\n",
    "TIMESTEPS = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "\n",
    "        #Actor\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        # Critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_action_and_value(self, state):\n",
    "        action_probs = self.actor(state) # pi(a|s) => left|right => [0.7, 0.3]\n",
    "        state_values = self.critic(state) # V(s), R+rV(s')-V(s) -> V(s) 때문에 있음\n",
    "\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample() # Action Prob의 분포로부터 샘플링한다.=> [0.7, 0.3] => 0번 Action\n",
    "        action_logprobs = dist.log_prob(action) #log(0.75)\n",
    "        entropy = dist.entropy() #entropy([0.7, 0.3])\n",
    "\n",
    "        return action, action_logprobs, state_values, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer():\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.dones = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(buffer, old_model, new_model, optimizer):\n",
    "\n",
    "    state = buffer.states[-1]\n",
    "    done = buffer.dones[-1]\n",
    "    with torch.no_grad():\n",
    "        discounted_rewards = 0 if done else old_model.get_action_and_value(torch.FloatTensor(state))[2].item()\n",
    "\n",
    "    returns = []\n",
    "    for reward in reversed(buffer.rewards):\n",
    "        discounted_rewards = reward + GAMMA * discounted_rewards\n",
    "        returns.insert(0, discounted_rewards)\n",
    "\n",
    "    advantages = torch.FloatTensor(returns) - torch.FloatTensor(buffer.state_values)\n",
    "\n",
    "    for _ in range(EPOCHS):\n",
    "        for idx in range(0, len(buffer.states), BATCH_SIZE):\n",
    "            batch_states = torch.FloatTensor(buffer.states[idx:idx+BATCH_SIZE])\n",
    "            batch_actions = torch.LongTensor(buffer.actions[idx:idx+BATCH_SIZE])\n",
    "\n",
    "            batch_returns = torch.FloatTensor(returns[idx:idx+BATCH_SIZE])\n",
    "            batch_advantages = torch.FloatTensor(advantages[idx:idx+BATCH_SIZE])\n",
    "\n",
    "            # 동일한 state에 대해 New model이라면 이렇게 했을 것이다\n",
    "            new_policy_logits = new_model.actor(batch_states)\n",
    "            values = new_model.critic(batch_states)\n",
    "            new_policy_dist = Categorical(logits=new_policy_logits)\n",
    "            new_log_probs = new_policy_dist.log_prob(batch_actions)\n",
    "            entropy = new_policy_dist.entropy()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                old_policy_logits = old_model.actor(batch_states)\n",
    "                old_policy_dist = Categorical(logits=old_policy_logits)\n",
    "                old_log_probs = old_policy_dist.log_prob(batch_actions)\n",
    "\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "            surrogate1 = ratios * batch_advantages\n",
    "            surrogate2 = torch.clamp(ratios, 1-EPSILON_CLIP, 1+EPSILON_CLIP) * batch_advantages\n",
    "            policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            value_loss = nn.MSELoss()(values.squeeze(), batch_returns)\n",
    "            entropy_loss = -ENTROPY_COEFF * entropy.mean()\n",
    "\n",
    "            loss = policy_loss + value_loss + entropy_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, episode reward 8.0\n",
      "Episode 2, episode reward 9.0\n",
      "Episode 3, episode reward 9.0\n",
      "Episode 4, episode reward 9.0\n",
      "Episode 5, episode reward 8.0\n",
      "Episode 6, episode reward 8.0\n",
      "Episode 7, episode reward 8.0\n",
      "Episode 8, episode reward 8.0\n",
      "Episode 9, episode reward 8.0\n",
      "Episode 10, episode reward 9.0\n",
      "Episode 11, episode reward 8.0\n",
      "Episode 12, episode reward 9.0\n",
      "Episode 13, episode reward 7.0\n",
      "Episode 14, episode reward 9.0\n",
      "Episode 15, episode reward 9.0\n",
      "Episode 16, episode reward 9.0\n",
      "Episode 17, episode reward 8.0\n",
      "Episode 18, episode reward 8.0\n",
      "Episode 19, episode reward 8.0\n",
      "Episode 20, episode reward 8.0\n",
      "Episode 21, episode reward 8.0\n",
      "Episode 22, episode reward 9.0\n",
      "Episode 23, episode reward 8.0\n",
      "Episode 24, episode reward 7.0\n",
      "Episode 25, episode reward 7.0\n",
      "Episode 26, episode reward 9.0\n",
      "Episode 27, episode reward 7.0\n",
      "Episode 28, episode reward 9.0\n",
      "Episode 29, episode reward 9.0\n",
      "Episode 30, episode reward 9.0\n",
      "Episode 31, episode reward 9.0\n",
      "Episode 32, episode reward 8.0\n",
      "Episode 33, episode reward 9.0\n",
      "Episode 34, episode reward 9.0\n",
      "Episode 35, episode reward 8.0\n",
      "Episode 36, episode reward 7.0\n",
      "Episode 37, episode reward 9.0\n",
      "Episode 38, episode reward 8.0\n",
      "Episode 39, episode reward 9.0\n",
      "Episode 40, episode reward 9.0\n",
      "Episode 41, episode reward 9.0\n",
      "Episode 42, episode reward 9.0\n",
      "Episode 43, episode reward 9.0\n",
      "Episode 44, episode reward 8.0\n",
      "Episode 45, episode reward 8.0\n",
      "Episode 46, episode reward 9.0\n",
      "Episode 47, episode reward 9.0\n",
      "Episode 48, episode reward 8.0\n",
      "Episode 49, episode reward 8.0\n",
      "Episode 50, episode reward 7.0\n",
      "Episode 51, episode reward 10.0\n",
      "Episode 52, episode reward 8.0\n",
      "Episode 53, episode reward 8.0\n",
      "Episode 54, episode reward 9.0\n",
      "Episode 55, episode reward 9.0\n",
      "Episode 56, episode reward 9.0\n",
      "Episode 57, episode reward 9.0\n",
      "Episode 58, episode reward 9.0\n",
      "Episode 59, episode reward 9.0\n",
      "Episode 60, episode reward 9.0\n",
      "Episode 61, episode reward 9.0\n",
      "Episode 62, episode reward 8.0\n",
      "Episode 63, episode reward 8.0\n",
      "Episode 64, episode reward 9.0\n",
      "Episode 65, episode reward 9.0\n",
      "Episode 66, episode reward 9.0\n",
      "Episode 67, episode reward 9.0\n",
      "Episode 68, episode reward 10.0\n",
      "Episode 69, episode reward 7.0\n",
      "Episode 70, episode reward 9.0\n",
      "Episode 71, episode reward 8.0\n",
      "Episode 72, episode reward 8.0\n",
      "Episode 73, episode reward 9.0\n",
      "Episode 74, episode reward 8.0\n",
      "Episode 75, episode reward 9.0\n",
      "Episode 76, episode reward 8.0\n",
      "Episode 77, episode reward 8.0\n",
      "Episode 78, episode reward 8.0\n",
      "Episode 79, episode reward 8.0\n",
      "Episode 80, episode reward 8.0\n",
      "Episode 81, episode reward 7.0\n",
      "Episode 82, episode reward 8.0\n",
      "Episode 83, episode reward 8.0\n",
      "Episode 84, episode reward 8.0\n",
      "Episode 85, episode reward 8.0\n",
      "Episode 86, episode reward 9.0\n",
      "Episode 87, episode reward 9.0\n",
      "Episode 88, episode reward 8.0\n",
      "Episode 89, episode reward 8.0\n",
      "Episode 90, episode reward 8.0\n",
      "Episode 91, episode reward 9.0\n",
      "Episode 92, episode reward 8.0\n",
      "Episode 93, episode reward 7.0\n",
      "Episode 94, episode reward 8.0\n",
      "Episode 95, episode reward 7.0\n",
      "Episode 96, episode reward 7.0\n",
      "Episode 97, episode reward 9.0\n",
      "Episode 98, episode reward 8.0\n",
      "Episode 99, episode reward 8.0\n",
      "Episode 100, episode reward 8.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "old_model = PPOActorCritic(state_dim, action_dim)\n",
    "new_model = PPOActorCritic(state_dim, action_dim)\n",
    "new_model.load_state_dict(old_model.state_dict())\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': new_model.actor.parameters(), 'lr': LEARNING_RATE},\n",
    "    {'params': new_model.critic.parameters(), 'lr': LEARNING_RATE},\n",
    "])\n",
    "\n",
    "buffer = RolloutBuffer()\n",
    "\n",
    "n_episodes = 100\n",
    "for episode in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    episode_reward = 0\n",
    "\n",
    "    buffer.clear()\n",
    "\n",
    "    for t in range(TIMESTEPS):\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, value, _ = old_model.get_action_and_value(state)\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "        # Store data\n",
    "        buffer.states.append(state.numpy())\n",
    "        buffer.actions.append(action.item())\n",
    "        buffer.rewards.append(reward)\n",
    "        buffer.dones.append(done)\n",
    "        buffer.log_probs.append(log_prob.item())\n",
    "        buffer.state_values.append(value.item())\n",
    "\n",
    "        state = torch.FloatTensor(next_state)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "            state = torch.FloatTensor(state)\n",
    "            break\n",
    "\n",
    "        # Train PPO\n",
    "        train_ppo(buffer, old_model, new_model, optimizer)\n",
    "        old_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        print(f\"\\rEpisode {episode + 1}, episode reward {episode_reward}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 \t\t Reward: 300.00\n",
      "Episode: 2 \t\t Reward: 300.00\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "max_ep_len = 300\n",
    "\n",
    "total_test_episodes = 2\n",
    "test_running_reward = 0\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode='human')\n",
    "\n",
    "for ep in range(1, total_test_episodes+1):\n",
    "    ep_reward = 0\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    for i in range(max_ep_len):\n",
    "        action_probs = new_model.actor(torch.FloatTensor(state))\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        state, reward, done, trancated, _ = env.step(action.numpy())\n",
    "        ep_reward += reward\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "        if done:\n",
    "            state, info = env.reset()\n",
    "            \n",
    "    test_running_reward += ep_reward\n",
    "    print(f\"Episode: {ep} \\t\\t Reward: {ep_reward:.2f}\")\n",
    "    ep_reward = 0\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_env",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
