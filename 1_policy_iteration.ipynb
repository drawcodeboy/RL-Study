{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    '''\n",
    "    Environment, Grid 4x4\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # State Transition Probability가 무조건 1이다.\n",
    "        self.state = np.zeros([4, 4]) # 4x4 Grid\n",
    "        self.action_space = [0, 1, 2, 3] # U, D, L, R\n",
    "        \n",
    "        self.goal_pos = {\n",
    "            'y': 3,\n",
    "            'x': 3\n",
    "        }\n",
    "        \n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        self.y_min, self.x_min, self.y_max, self.x_max = 0, 0, 3, 3\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.zeros([4, 4])\n",
    "        return self.state\n",
    "    \n",
    "    def s_next(self, s_t, a_t):\n",
    "        '''\n",
    "        Args:\n",
    "            s_t: (tuple) Environment에서 위치를 의미한다. (row, col)형태로 들어온다.\n",
    "            a_t: [0, 3] 범위의 int 값으로 어떤 Action을 취하는지 의미한다.\n",
    "        Return:\n",
    "            pos: (list) Action에 따라 state를 이동한 후, state의 위치를 list에 담아 리턴.\n",
    "        '''\n",
    "        pos = list(s_t) # Tuple to List for edit\n",
    "        \n",
    "        # Action에 따라 state를 이동하면서 Boundary를 넘지 않도록 한다.\n",
    "        if a_t == 0:\n",
    "            pos[0] = max(s_t[0]-1, self.y_min)\n",
    "        elif a_t == 1:\n",
    "            pos[0] = min(s_t[0]+1, self.y_max)\n",
    "        elif a_t == 2:\n",
    "            pos[1] = max(s_t[1]-1, self.x_min)\n",
    "        elif a_t == 3:\n",
    "            pos[1] = min(s_t[1]+1, self.x_max)\n",
    "        else:\n",
    "            raise AssertionError(\"Invalid Action\")\n",
    "        \n",
    "        return pos\n",
    "    \n",
    "    def reward(self, s_t, a_t, s_next):\n",
    "        '''\n",
    "        state(s_t)가 Goal에 들어가면 Reward 0 리턴\n",
    "        그게 아니라면 모든 Reward는 -1\n",
    "        '''\n",
    "        if (s_t[0] == self.goal_pos['y'] and s_t[1] == self.goal_pos['x']):\n",
    "            return 0\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "* 어느 Policy를 평가하려면, 해당 Policy를 통해 각 State의 Value function을 구하면\n",
    "Policy의 가치가 어느 정도인지 판단할 수 있다.\n",
    "\n",
    "* 그러면, Value function은 어떻게 구할 것인가? Bellman equation을 통해 다음 state의\n",
    "Value function과의 점화식을 도출했다면 recursive하게 구할 수 있다.\n",
    "\n",
    "* 하지만, recursive하게 구하는 것은 너무 비효율적이고 계산하기 어렵다.\n",
    "그래서, 제안된 방식이 iterative한 방식인데 이는 더 직관적이며 효율적이다.\n",
    "\n",
    "* 또한, iterative한 방식이 작동할 수 있는 이유는 (각 V(s)가 특정 값에 근사할 수 있는 이유는)\n",
    "Bellman equation의 Contraction mapping이라는 성질 때문이다.\n",
    "이는 어떤 값이 갱신 될 때, 값들이 점점 더 가까워지도록 만드는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy):\n",
    "    '''\n",
    "    위 Markdown 참고\n",
    "    Contraction Mapping에 의해 특정 값에 근사되기 때문에 \n",
    "    이 변화 값이 theta보다 작을 때, 멈추도록 한다.\n",
    "    '''\n",
    "    delta = 1 # 이건 Initial Value, 1인 이유나 이런 거 상괍 없음.\n",
    "    theta = 0.01\n",
    "    \n",
    "    loop_count = 0\n",
    "    while delta >= theta:\n",
    "        delta = 0 # 여기서 초기화 되기 때문에\n",
    "        y, x = env.state.shape\n",
    "        new_state = np.zeros((y, x)) # 여기에 각 State에 대한 V(s) 넣을 거임\n",
    "        \n",
    "        for i in range(y*x-1):\n",
    "            # V(s)를 구할 State 가져오기\n",
    "            v_s = 0\n",
    "            s_t = np.divmod(i, y)\n",
    "            \n",
    "            for a_t in env.action_space:\n",
    "                # 탐색 중인 State의 Policy 가져오기 + 특정 Action에 대한 Prob\n",
    "                pi_a = policy[i][a_t]\n",
    "                p_ss = 1.0 # state transition probability has only 1\n",
    "                \n",
    "                # Action에 따른 다음 State와 현재 state에 대한 reward\n",
    "                s_t1 = env.s_next(s_t, a_t)\n",
    "                reward = env.reward(s_t, a_t, s_t1)\n",
    "                \n",
    "                v_s = v_s + pi_a * p_ss * (reward + env.gamma * env.state[s_t1[0], s_t1[1]])\n",
    "                \n",
    "            new_state[s_t[0], s_t[1]] = v_s\n",
    "        \n",
    "        value_delta = np.sum(np.abs(new_state - env.state))\n",
    "        env.state = new_state # V(s) 갱신\n",
    "        \n",
    "        delta = max(delta, value_delta)\n",
    "        \n",
    "        loop_count += 1\n",
    "        print(f\"(Policy Evaluation)[{loop_count:03d}] Delta: {delta}\")\n",
    "        \n",
    "    return env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Policy Evaluation)[001] Delta: 15.0\n",
      "(Policy Evaluation)[002] Delta: 13.049999999999999\n",
      "(Policy Evaluation)[003] Delta: 11.44125\n",
      "(Policy Evaluation)[004] Delta: 10.046531250000005\n",
      "(Policy Evaluation)[005] Delta: 8.836846875000003\n",
      "(Policy Evaluation)[006] Delta: 7.777860468750003\n",
      "(Policy Evaluation)[007] Delta: 6.850606640625003\n",
      "(Policy Evaluation)[008] Delta: 6.0358126133056675\n",
      "(Policy Evaluation)[009] Delta: 5.319780200925291\n",
      "(Policy Evaluation)[010] Delta: 4.6894928129728095\n",
      "(Policy Evaluation)[011] Delta: 4.134661461056415\n",
      "(Policy Evaluation)[012] Delta: 3.6458383469337097\n",
      "(Policy Evaluation)[013] Delta: 3.215154091310108\n",
      "(Policy Evaluation)[014] Delta: 2.8355211386587618\n",
      "(Policy Evaluation)[015] Delta: 2.500874475079919\n",
      "(Policy Evaluation)[016] Delta: 2.2058087148637213\n",
      "(Policy Evaluation)[017] Delta: 1.9456324929596231\n",
      "(Policy Evaluation)[018] Delta: 1.7161873178144953\n",
      "(Policy Evaluation)[019] Delta: 1.5138369561042664\n",
      "(Policy Evaluation)[020] Delta: 1.3353668589267293\n",
      "(Policy Evaluation)[021] Delta: 1.1779549588006892\n",
      "(Policy Evaluation)[022] Delta: 1.0391096376920173\n",
      "(Policy Evaluation)[023] Delta: 0.9166387795440762\n",
      "(Policy Evaluation)[024] Delta: 0.808608063022036\n",
      "(Policy Evaluation)[025] Delta: 0.7133136768789745\n",
      "(Policy Evaluation)[026] Delta: 0.6292525355057403\n",
      "(Policy Evaluation)[027] Delta: 0.555099811467036\n",
      "(Policy Evaluation)[028] Delta: 0.4896868518374946\n",
      "(Policy Evaluation)[029] Delta: 0.43198322360596997\n",
      "(Policy Evaluation)[030] Delta: 0.3810799797465956\n",
      "(Policy Evaluation)[031] Delta: 0.3361755105044102\n",
      "(Policy Evaluation)[032] Delta: 0.29656270880309954\n",
      "(Policy Evaluation)[033] Delta: 0.26161789493827836\n",
      "(Policy Evaluation)[034] Delta: 0.230790907414252\n",
      "(Policy Evaluation)[035] Delta: 0.2035964610753549\n",
      "(Policy Evaluation)[036] Delta: 0.17960646908676203\n",
      "(Policy Evaluation)[037] Delta: 0.15844330998670397\n",
      "(Policy Evaluation)[038] Delta: 0.13977386685248572\n",
      "(Policy Evaluation)[039] Delta: 0.12330428524069781\n",
      "(Policy Evaluation)[040] Delta: 0.10877534056804805\n",
      "(Policy Evaluation)[041] Delta: 0.09595835805373287\n",
      "(Policy Evaluation)[042] Delta: 0.0846516102487822\n",
      "(Policy Evaluation)[043] Delta: 0.07467714179475049\n",
      "(Policy Evaluation)[044] Delta: 0.06587796711618843\n",
      "(Policy Evaluation)[045] Delta: 0.058115599444730215\n",
      "(Policy Evaluation)[046] Delta: 0.05126787054629034\n",
      "(Policy Evaluation)[047] Delta: 0.04522700782200939\n",
      "(Policy Evaluation)[048] Delta: 0.03989793781823803\n",
      "(Policy Evaluation)[049] Delta: 0.03519678983168628\n",
      "(Policy Evaluation)[050] Delta: 0.031049575775250027\n",
      "(Policy Evaluation)[051] Delta: 0.0273910256788108\n",
      "(Policy Evaluation)[052] Delta: 0.024163560382413962\n",
      "(Policy Evaluation)[053] Delta: 0.021316385312514008\n",
      "(Policy Evaluation)[054] Delta: 0.018804691031892418\n",
      "(Policy Evaluation)[055] Delta: 0.016588948003875004\n",
      "(Policy Evaluation)[056] Delta: 0.014634284452585966\n",
      "(Policy Evaluation)[057] Delta: 0.012909937536162097\n",
      "(Policy Evaluation)[058] Delta: 0.011388769187978376\n",
      "(Policy Evaluation)[059] Delta: 0.010046839008748698\n",
      "(Policy Evaluation)[060] Delta: 0.008863027484746766\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "\n",
    "policy = [np.array([0.25]*4) for _ in range(16)]\n",
    "\n",
    "res = policy_evaluation(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.35537351 -9.21356672 -8.96998585 -8.74548904]\n",
      " [-9.21356672 -8.96858758 -8.49571346 -7.96588579]\n",
      " [-8.96998585 -8.49571346 -7.41217763 -5.75452192]\n",
      " [-8.74548904 -7.96588579 -5.75452192  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(res.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement (Greedy)\n",
    "* $\\pi'(s)=argmax_aq_{pi}(s,a)$, 이 식이 전부다.\n",
    "$$ \\pi'(s)=\\begin{cases}1 & \\text{if}\\:a=argmax_aq_{\\pi_k}(s,a)\\\\0 & \\text{else}\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy):\n",
    "    gamma = 1.0\n",
    "    y, x = env.state.shape\n",
    "    for i in range(y*x):\n",
    "        s_t = np.divmod(i, y)\n",
    "        action_values = np.zeros(len(env.action_space))\n",
    "        \n",
    "        # 어느 State에서 각 Action에 대한 Action-Value 구하기\n",
    "        for a in env.action_space:\n",
    "            action_value = 0\n",
    "            p_s_next = 1.0 # state transition probability\n",
    "            s_next = env.s_next(s_t, a)\n",
    "            reward = env.reward(s_t, a, s_next)\n",
    "            \n",
    "            # 여기서 Policy Evaluation을 통해 각 State의 Value function을 \n",
    "            # 구해두었기 때문에 Action Value를 구할 수 있다.\n",
    "            action_value = p_s_next * (reward + env.gamma * env.state[s_next[0], s_next[1]])\n",
    "            action_values[a] = action_value\n",
    "        \n",
    "        # 여기가 핵심(위 Markdown의 수식)\n",
    "        a_max = action_values.argmax()\n",
    "        policy[i][:] = 0\n",
    "        policy[i][a_max] = 1\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "* Evaluation을 통해 현재 Policy에 대한 평가를 하고,\n",
    "* Improvement를 통해 현재 Policy를 더 나은 방향으로 개선시킨다.\n",
    "* 이 과정을 반복하면서 Policy Evaluation을 통해 나온 Value function 값이 더 이상 변화가 되지 않는다면,\n",
    "* 제일 Optimal한 Policy가 되었다는 의미이기 때문에 Iteration을 멈춘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Policy Evaluation)[001] Delta: 15.0\n",
      "(Policy Evaluation)[002] Delta: 13.049999999999999\n",
      "(Policy Evaluation)[003] Delta: 11.44125\n",
      "(Policy Evaluation)[004] Delta: 10.046531250000005\n",
      "(Policy Evaluation)[005] Delta: 8.836846875000003\n",
      "(Policy Evaluation)[006] Delta: 7.777860468750003\n",
      "(Policy Evaluation)[007] Delta: 6.850606640625003\n",
      "(Policy Evaluation)[008] Delta: 6.0358126133056675\n",
      "(Policy Evaluation)[009] Delta: 5.319780200925291\n",
      "(Policy Evaluation)[010] Delta: 4.6894928129728095\n",
      "(Policy Evaluation)[011] Delta: 4.134661461056415\n",
      "(Policy Evaluation)[012] Delta: 3.6458383469337097\n",
      "(Policy Evaluation)[013] Delta: 3.215154091310108\n",
      "(Policy Evaluation)[014] Delta: 2.8355211386587618\n",
      "(Policy Evaluation)[015] Delta: 2.500874475079919\n",
      "(Policy Evaluation)[016] Delta: 2.2058087148637213\n",
      "(Policy Evaluation)[017] Delta: 1.9456324929596231\n",
      "(Policy Evaluation)[018] Delta: 1.7161873178144953\n",
      "(Policy Evaluation)[019] Delta: 1.5138369561042664\n",
      "(Policy Evaluation)[020] Delta: 1.3353668589267293\n",
      "(Policy Evaluation)[021] Delta: 1.1779549588006892\n",
      "(Policy Evaluation)[022] Delta: 1.0391096376920173\n",
      "(Policy Evaluation)[023] Delta: 0.9166387795440762\n",
      "(Policy Evaluation)[024] Delta: 0.808608063022036\n",
      "(Policy Evaluation)[025] Delta: 0.7133136768789745\n",
      "(Policy Evaluation)[026] Delta: 0.6292525355057403\n",
      "(Policy Evaluation)[027] Delta: 0.555099811467036\n",
      "(Policy Evaluation)[028] Delta: 0.4896868518374946\n",
      "(Policy Evaluation)[029] Delta: 0.43198322360596997\n",
      "(Policy Evaluation)[030] Delta: 0.3810799797465956\n",
      "(Policy Evaluation)[031] Delta: 0.3361755105044102\n",
      "(Policy Evaluation)[032] Delta: 0.29656270880309954\n",
      "(Policy Evaluation)[033] Delta: 0.26161789493827836\n",
      "(Policy Evaluation)[034] Delta: 0.230790907414252\n",
      "(Policy Evaluation)[035] Delta: 0.2035964610753549\n",
      "(Policy Evaluation)[036] Delta: 0.17960646908676203\n",
      "(Policy Evaluation)[037] Delta: 0.15844330998670397\n",
      "(Policy Evaluation)[038] Delta: 0.13977386685248572\n",
      "(Policy Evaluation)[039] Delta: 0.12330428524069781\n",
      "(Policy Evaluation)[040] Delta: 0.10877534056804805\n",
      "(Policy Evaluation)[041] Delta: 0.09595835805373287\n",
      "(Policy Evaluation)[042] Delta: 0.0846516102487822\n",
      "(Policy Evaluation)[043] Delta: 0.07467714179475049\n",
      "(Policy Evaluation)[044] Delta: 0.06587796711618843\n",
      "(Policy Evaluation)[045] Delta: 0.058115599444730215\n",
      "(Policy Evaluation)[046] Delta: 0.05126787054629034\n",
      "(Policy Evaluation)[047] Delta: 0.04522700782200939\n",
      "(Policy Evaluation)[048] Delta: 0.03989793781823803\n",
      "(Policy Evaluation)[049] Delta: 0.03519678983168628\n",
      "(Policy Evaluation)[050] Delta: 0.031049575775250027\n",
      "(Policy Evaluation)[051] Delta: 0.0273910256788108\n",
      "(Policy Evaluation)[052] Delta: 0.024163560382413962\n",
      "(Policy Evaluation)[053] Delta: 0.021316385312514008\n",
      "(Policy Evaluation)[054] Delta: 0.018804691031892418\n",
      "(Policy Evaluation)[055] Delta: 0.016588948003875004\n",
      "(Policy Evaluation)[056] Delta: 0.014634284452585966\n",
      "(Policy Evaluation)[057] Delta: 0.012909937536162097\n",
      "(Policy Evaluation)[058] Delta: 0.011388769187978376\n",
      "(Policy Evaluation)[059] Delta: 0.010046839008748698\n",
      "(Policy Evaluation)[060] Delta: 0.008863027484746766\n",
      "->(Policy Improvement) [001] Delta: 36.28441512584901\n",
      "(Policy Evaluation)[001] Delta: 15.0\n",
      "(Policy Evaluation)[002] Delta: 11.7\n",
      "(Policy Evaluation)[003] Delta: 8.100000000000001\n",
      "(Policy Evaluation)[004] Delta: 4.3740000000000006\n",
      "(Policy Evaluation)[005] Delta: 1.968300000000001\n",
      "(Policy Evaluation)[006] Delta: 0.59049\n",
      "(Policy Evaluation)[007] Delta: 0\n",
      "->(Policy Improvement) [002] Delta: 21.354725125849008\n",
      "(Policy Evaluation)[001] Delta: 15.0\n",
      "(Policy Evaluation)[002] Delta: 11.7\n",
      "(Policy Evaluation)[003] Delta: 8.100000000000001\n",
      "(Policy Evaluation)[004] Delta: 4.3740000000000006\n",
      "(Policy Evaluation)[005] Delta: 1.968300000000001\n",
      "(Policy Evaluation)[006] Delta: 0.59049\n",
      "(Policy Evaluation)[007] Delta: 0\n",
      "->(Policy Improvement) [003] Delta: 0\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "\n",
    "policy = [np.array([0.25]*4) for _ in range(16)]\n",
    "\n",
    "# Policy Iteration\n",
    "value_vector = np.zeros(16)\n",
    "delta, Delta, cnt = 5, 0, 0\n",
    "\n",
    "while True:\n",
    "    env.reset() # Value function 초기화의 목적\n",
    "    new_value_vector = policy_evaluation(env, policy)\n",
    "    Delta = 0\n",
    "    Delta = max(Delta, np.sum(np.abs(new_value_vector[0]-value_vector[0])))\n",
    "    \n",
    "    if Delta >= delta:\n",
    "        # Update 더 해야 함.\n",
    "        value_vector = new_value_vector # 갱신\n",
    "        policy = policy_improvement(env, policy)\n",
    "        cnt += 1\n",
    "        print(f\"->(Policy Improvement) [{cnt:03d}] Delta: {Delta}\")\n",
    "    else:\n",
    "        # Update 그만\n",
    "        cnt += 1\n",
    "        print(f\"->(Policy Improvement) [{cnt:03d}] Delta: {Delta}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_env",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
