{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo Learning\n",
    "* Model-Free Learning으로, State transition Probability와 Reward를 모를 때 학습할 수 있는 기법을 의미한다.\n",
    "## Evaluation\n",
    "* $V^{\\pi}(s_t)=E_{\\pi}[G_t\\mid S_t=s_t]$에서 $G_t^{\\pi}=R_{t+1}^{\\pi}+\\gamma R_{t+2}^{\\pi}+\\cdots+\\gamma^{T-1}R_{t+T}^{\\pi}$는\n",
    "* Expectation에 의해 밖으로 꺼내면 $R_{t+1}^{\\pi}=P_{ss'}^{\\pi}R_{t+1}$이 되는데, 이 때 State transition probability $P_{ss'}^{\\pi}$와 Reward $R_{t+1}$를 알 수가 없다.\n",
    "* * *\n",
    "* 이에 대해 결론적으로, $V^{\\pi}(s_t)$를 구하는 것이 목적이기 때문에 이는 Law of Large Numbers로 구할 수 있다.\n",
    "* 무한 번에 가까운 에피소드를 발생시켜서 나오는 무한 개에 가까운 Value function의 평균을 구하면 모든 State의 Value function을 구할 수 있다.\n",
    "* 이 과정에서 State transition probability를 모르는 건 상관이 없다. 현재의 Policy에 의존하여 <b>추론</b>을 할 것이기 때문에 괜찮다.\n",
    "* * * \n",
    "* 또한, Value function 값을 계속 누적으로 가지고 있는 것은 비효율적이기 때문에 Incremental Mean을 사용해서 Episode가 끝나면 즉각적으로 평균값에 업데이트한다.\n",
    "## Improvement\n",
    "* 하지만, Policy를 update(Improvement)하려면 Action Value가 필요한데 Evaluation에서는 State Value를 구하는 것을 채택했다.\n",
    "* State Value로부터 Action Value를 도출하는 방법이 있지만, 이것은 Reward와 State transition probability를 필요로 한다.\n",
    "* 이것은 알 수가 없기 때문에, 애초에 Evaluation 할 때부터 <u>State Value가 아니라 Action Value를 구해버리면 된다.</u>\n",
    "    * 16개의 State와 4개의 Action이 있다고 했을 때, 16개 구할 거 64개 구하는 것과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_env",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
